# LLaMA: Open and Efficient Foundation Language Models

> By ***Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal,
Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin,
Edouard Grave, Guillaume Lample***

[Research Paper Link](https://arxiv.org/abs/2302.13971)

### Abstract
- LLaMA: collection of foundation language models ranging from 7b to 65B parameters.
- Trained in trillions of tokens.
- Shows it is possible to train state-of-the-art models using publicaly available datasets.
- LLaMA-13B outperforms GPT-3 (1757B).
- LLaMA-65B competitive with Chinchilla-70B and PaLM-540B.

### Introduction
-
