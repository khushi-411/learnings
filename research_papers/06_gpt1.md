# Improving Language Understanding by Generative Pre-Training <sup>[link](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)</sup>

> By Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever

### Abstract
- Unlabeled text corpara are very abundant compared to labeled data for learning specific tasks, it makes it challenging for the model to learn.
- Generative pre-training can solve this problem, followed by the discriminative fine-tuning on each specific taks.
- In contrast to previous approcahes, make use of tasks aware of input transformation during fine-tuning.
    - To achieve effective transfer with minimal changes in model.
- Improvements: 8.9% on commonsense reasoning,  5.7% on question answering , 1.5% on textual entailment.

### Introduction
- Many models require large amount of labeled data, this restricts their ability to perform tasks.
- For these cases, models can take information from unlabeled data, but this can be time consuming and expensive.
- Learning from unsupervised learning can provide boost. Eg: use of word embeddings in pre-trained tasks.
- Why word-level information challenging from unlabeled text?
    - What kind of optimization objectives are effective at learning.
    - No general way to find the most effective way to transfer the learned representations to the target tasks.
- Existing techniques: combination of tasks-specific changes to the model architecture.
- This paper explores **semi-supervised approach** (unsupervised pre-traning and supervised fine-tuning).
- Goal: Learn universal representation that transfers with little apadtation of wide range of tasks.
- Steps:
    - Language modeling objective on the unlabeled data to learn the initial parameters of the neural netwrok model.
    - Then adapt these parameters to a target tasks using the corresponding supervised objective.
- **Transformer** model is used.
- Evaluation approaches are natural language inference, question answering, semantic similarity, and text classification.
- Results:
    - Improved 9 out of 12 state of arts.
    - Improvements of 8.9% on commonsense reasoning, 5.7% on question answering, 1.5% on textual entailment, and 5.5% on GLUE multi-task benchmark.

### Related Work
1. **Semi-supervised learning for NLP**
    - Other models that use this approach: sequence labeling, text classification, etc.
    - Earlier apporaches: use unlabeled data to compute word-level or phrase-level stats, which are then used in a supervised model.
    - These approaches mainly transfer word-level information, but GPT is wants to capture higher-level semantics.
2. **Unsupervised pre-training**
    - Special case of semi-supervised learning, where goal is to find a good initialization point.
    - Earlier works: Image classification, regression tasks, speech recognition, entity disambiguation, and machine translation.
    - Research found that pre-training acts like a regularization scheme.
    - To improve text classification by Dai, Howard, and Ruder; closest approach to this paper:
        - pre-training a neural network using a language modeling objective.
        - fine-tuning it on a target task with supervision.
    - In this paper:
        - Hidden representations from a pre-trained language or machine translation model as auxiliary features.
        - Traning supervised model on the target task.
3. **Auxiliary training objectives**
    - It's an alternative form of semi-supervised learning.
    - Earlier works: POS tagging, chunking, named entity recognition, and language modeling.
    - Added this by Rei, to target task objective which demonstrated performance gains on sequence labeling tasks.

### Framework
Training process consists of two stages:
- Learning high-capacity language model on large corpus of text.
- Then fine-tuning stage

1. **Unsupervised pre-training**
- To maximize the likelihood:

$$
L_1(U) = \sum_i \log P(u_i | u_{i-k},..., u_{i-1}; \Theta)
$$

- Here, $k$ is the size of the context window, $P$ is the conditional probability, parameters $\Theta$.
- Parameters are trained using stochastic gradient descent.
- Used multi-layer Transformer decoder. Model applies:
    - Multi-headed self-attention operation over the input context tokens.
    - Then feedforward layers to produce output distribution over target tokens.

$$
h_0 = U We + Wp
$$

$$
h_l = \text{transformer\_block}(h_{l-1}) \quad \forall i \in [1, n]
$$

$$
P(u) = \text{softmax}(h_n^T W^T_e)
$$

- Here,
    - $U = {u_1, u_2,..., u_n}$: Vector tokens
    - $n$: number of layers
    - $W_e$: token embedding matrix
    - $W_p$: position embedding matrix

2. **Supervised fine-tuning**
- After training the model as mentioned above the researchers adopted the supervised training approach.
- Here,
    - $C$: labeled datasets
    - $x^1, x^2,..., x^m$: sequence of inputs
    - $y$: label
    - $h_l^m$: trnsformer block's activation
    -  $W_y$: linear layer parameter

$$
P(y|x_1, \ldots, x_m) = \text{softmax}(h_{ml} W_y)
$$

This gives the objective to maximize:

$$
L_2(C) = \sum_{(x, y)} \log P(y|x_1, \ldots, x_m)
$$

- If we include, auxiliary objective to the fine-tuning language model, helped in:
    - improving generalization of the supervised model
    - accelerating convergence
- The researchers optimize the following objective:

$$
L_3(C) = L_2(C) + \lambda \cdot L_1(C)
$$

3. **Task-specific input transformations**
- For text classification tasks like above we could directly fine-tune our model as mentioned above.
- Since, pre-trained model was trained on contiguous sequences of text, we require to make changes.
- Hence, this introduces huge amount of task-specific customization and does not use transfer learning for these.
    - Use **traversal-style approach**: convert structured inputs into an ordered sequence that our pre-trained model can process.
- Input transformation allows to avoid extensive changes to the architecture across tasks. The transformations are:
    - Random initialized start and end tokens.
    - **Textual entailment**: Concatenate the premise $p$ and hypothesis $h$ token sequences, with a delimiter token in between.
    - **Similarity**: For such tasks, the is no ordering of the two sentences being compared. They modified the input sequence to contain both possible sentence orderings and process each independently to produce sequence representations.
    - **Question Answering and Commonsense Reasoning**: Concatenate the document context and question with each possible answer. Each of the sequence are processed independently with the model and normalized via a softmax layer to produce the output.

### Experiments

#### Setup
1. **Unsupervised pre-training**
2. **Model specifications**
3. **Fine-tuning details**

#### Supervised fine-tuning
1. **Natural Language Inference**
2. **Question answering and commonsense reasoning**
3. **Semantic Similarity**
4. **Classification**

### Analysis
1. **Impact of number of layers transferred**
    - Impact of variable number of layers from unsupervised pre-training to the supervised target tasks.
    - Results standerd result that transferring embeddings improves performance.
    - Each transformer layer provides upto 9% benefits.
2. **Zero-shot Behaviors**
    - Understand why language model pre-training of transformers is effective.
    - Observation: performance of these heuristics is stable and steadily increases over training suggesting that generative pretraining supports the learning of a wide variety of task relevant functionality.
    - LSTM has high variance in its zero-shot training.
    - Transformer as an inductive bias nature which helps it in transfer.
3. **Ablation studies**: performed three different studies.
    - Without auxilary LM objective during fine-tuning. Overvation: it helps on the NLI tasks and QQP; larger datasets benefits.
    - Effect of transformer. Observation: 5.6 average score drop when using LSTM instead of transformer.
    - Comapre transformer architecture directly on the supervised target tasks, without pre-training. This causes 14.8% decrease then the proposed solution.

### Conclusion
- By pre-training
on a diverse corpus with long stretches of contiguous text our model acquires significant world
knowledge and ability to process long-range dependencies which are then successfully transferred to
solving discriminative tasks such as question answering, semantic similarity assessment, entailment
determination, and text classification, improving the state of the art on 9 of the 12 datasets we
study.
- Using unsupervised (pre-)training to boost performance.
- Suggests significant improvement possible.
