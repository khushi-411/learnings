# Improving Language Understanding by Generative Pre-Training

> By ***Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever***

[Research Paper Link](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

### Abstract
- Unlabeled text corpora are very abundant compared to labeled data for learning specific tasks, which makes it challenging for the model to learn.
- Generative pre-training can solve this problem, followed by discriminative fine-tuning on each specific task.
- In contrast to previous approaches, make use of tasks aware of input transformation during fine-tuning.
    - To achieve effective transfer with minimal changes in the model.
- Improvements: $8.9%$ on commonsense reasoning,  $5.7%$ on question answering , $1.5%$ on textual entailment.

### Introduction
- Many models require large amounts of labeled data, which restricts their ability to perform tasks.
- For these cases, models can take information from unlabeled data, but this can be time-consuming and expensive.
- Learning from unsupervised learning can provide a boost. E.g., the use of word embeddings in pre-trained tasks.
- Why is word-level information challenging from unlabeled text?
    - What kind of optimization objectives are effective at learning?
    - No general way to find the most effective way to transfer the learned representations to the target tasks.
- Existing techniques: a combination of tasks-specific changes to the model architecture.
- This paper explores **semi-supervised approach** (unsupervised pre-training and supervised fine-tuning).
- Goal: Learn universal representation that transfers with little adaptation of a wide range of tasks.
- Steps:
    - Language modeling objective on the unlabeled data to learn the initial parameters of the neural netwrok model.
    - Then, adapt these parameters to target tasks using the corresponding supervised objective.
- **Transformer** model is used.
- Evaluation approaches are natural language inference, question answering, semantic similarity, and text classification.
- Results:
    - Improved $9$ out of $12$ state of arts.
    - Improvements of $8.9%$ on commonsense reasoning, $5.7%$ on question answering, $1.5%$ on textual entailment, and $5.5%$ on GLUE multi-task benchmark.

### Related Work
1. **Semi-supervised learning for NLP**
    - Other models that use this approach are sequence labeling, text classification, etc.
    - Earlier approaches: use unlabeled data to compute word-level or phrase-level stats, which are then used in a supervised model.
    - These approaches mainly transfer word-level information, but GPT wants to capture higher-level semantics.
2. **Unsupervised pre-training**
    - Special case of semi-supervised learning, where the goal is to find a good initialization point.
    - Earlier works: Image classification, regression tasks, speech recognition, entity disambiguation, and machine translation.
    - Research found that pre-training acts like a regularization scheme.
    - To improve text classification by Dai, Howard, and Ruder; closest approach to this paper:
        - pre-training a neural network using a language modeling objective.
        - fine-tuning it on a target task with supervision.
    - In this paper:
        - Hidden representations from a pre-trained language or machine translation model as auxiliary features.
        - Training supervised model on the target task.
3. **Auxiliary training objectives**
    - It's an alternative form of semi-supervised learning.
    - Earlier works: POS tagging, chunking, named entity recognition, and language modeling.
    - Added this by Rei to target task objective, which demonstrated performance gains on sequence labeling tasks.

### Framework
The training process consists of two stages:
- Learning high-capacity language models on a large corpus of text.
- Then fine-tuning stage

1. **Unsupervised pre-training**
- To maximize the likelihood:

$$
L_1(U) = \sum_i \log P(u_i | u_{i-k},..., u_{i-1}; \Theta)
$$

- Here, $k$ is the size of the context window, $P$ is the conditional probability, parameters $\Theta$.
- Parameters are trained using stochastic gradient descent.
- Used multi-layer Transformer decoder. Model applies:
    - Multi-headed self-attention operation over the input context tokens.
    - Then, feedforward layers to produce output distribution over target tokens.

$$
h_0 = U We + Wp
$$

$$
h_l = \text{transformerBlock}(h_{l-1}) \quad \forall i \in [1, n]
$$

$$
P(u) = \text{softmax}(h_n^T W^T_e)
$$

- Here,
    - $U = {u_1, u_2,..., u_n}$: Vector tokens
    - $n$: number of layers
    - $W_e$: token embedding matrix
    - $W_p$: position embedding matrix

2. **Supervised fine-tuning**
- After training the model as mentioned above, the researchers adopted the supervised training approach.
- Here,
    - $C$: labeled datasets
    - $x^1, x^2,..., x^m$: sequence of inputs
    - $y$: label
    - $h_l^m$: transformer block's activation
    -  $W_y$: linear layer parameter

$$
P(y|x_1, \ldots, x_m) = \text{softmax}(h_{ml} W_y)
$$

This gives the objective of maximizing:

$$
L_2(C) = \sum_{(x, y)} \log P(y|x_1, \ldots, x_m)
$$

- If we include an auxiliary objective to the fine-tuning language model, it helped in:
    - improving the generalization of the supervised model
    - accelerating convergence
- The researchers optimize the following objective:

$$
L_3(C) = L_2(C) + \lambda \cdot L_1(C)
$$

3. **Task-specific input transformations**
- For text classification tasks like the above, we could directly fine-tune our model as mentioned above.
- Since the pre-trained model was trained on contiguous sequences of text, we are required to make changes.
- Hence, this introduces a huge amount of task-specific customization and does not use transfer learning for these.
    - Use **traversal-style approach**: convert structured inputs into an ordered sequence that our pre-trained model can process.
- Input transformation allows us to avoid extensive changes to the architecture across tasks. The transformations are:
    - Random initialized start and end tokens.
    - **Textual entailment**: Concatenate the premise $p$ and hypothesis $h$ token sequences, with a delimiter token in between.
    - **Similarity**: For such tasks, there is no ordering of the two sentences being compared. They modified the input sequence to contain both possible sentence orderings and processed each independently to produce sequence representations.
    - **Question Answering and Commonsense Reasoning**: Concatenate the document context and question with each possible answer. Each of the sequences is processed independently with the model and normalized via a softmax layer to produce the output.

### Experiments

#### Setup
1. **Unsupervised pre-training**
    - BooksCorpus and Word benchmark datasets for training.
    - Model achieves a very low token level perplexity of $18.4$.
2. **Model specifications**
    - Transformer work.
    - $12$-layer decoder only transformer with masked self-attention heads ($768$ dimensional states and $12$ attention heads).
    - For position-wise feed-forward network, $3072$ dimensional inner states.
    - Adam optimization.
    - Max learning rate = $2.5e-4$; it was linearly increased over first $2000$ updates.
    - Trained for $100$ epochs on minibataches of size $64$ randomly, contiguous sequences of $512$ tokens.
    - Layer norm was used over the model.
    - Weight initialization $N(0, 0.02)$.
    - Regularization rate: $0.1$.
    - Modified version of L3 regularization, $w = 0.01$ on all non bias or gain weights.
    - Activation function: Gaussian Linear Unit.
    - Used learned position embeddings instead of a sinusoidal version.
    - *ftfy* library to clean the raw text in the dataset, standardize punctuation and whitespace, and use *spaCy* tokenizer.
3. **Fine-tuning details**
    - Reuse the hyperparameter settings from unsupervised pre-training.
    - Dropout rate = $0.1$
    - Learning rate = $6.25e-5$, batch-size = $32$
    - Use learning rate decay schedule with warmup over $0.2%$ of training.
    - $\Lambda = 0.5$

#### Supervised fine-tuning
1. **Natural Language Inference (NLI)**
    - This task involves reading a pair of sentences and judging the relationship between them.
    - This is a challenging task that involves vague tasks.
    - Datasets used for evaluation are image captions, translated speech, popular fiction, government reports, Wikipedia articles, science exams, or news articles.
2. **Question answering and commonsense reasoning**
    - Requires aspects of single and multi-sentence reasoning.
    - RACE and Story Cloze Datasets are used.
    - This demonstrates the model's ability to use long-range contexts effectively.
3. **Semantic Similarity**
    - Involves whether the two statements are equivalent or not.
    - Datasets used were Microsoft Paraphrase corpus, Quora Question Pairs, and Semantic Textual similarity benchmarks.
4. **Classification**
    - Evaluate text classification tasks.
    - Datasets used were CoLA and SST-2.

### Analysis
1. **Impact of number of layers transferred**
    - Impact of a variable number of layers from unsupervised pre-training to the supervised target tasks.
    - Results: a standard result is that transferring embeddings improves performance.
    - Each transformer layer provides up to $9%$ benefits.
2. **Zero-shot Behaviors**
    - Understand why language model pre-training of transformers is effective.
    - Observation: performance of these heuristics is stable and steadily increases over training, suggesting that generative pretraining supports the learning of a wide variety of task-relevant functionality.
    - LSTM has high variance in its zero-shot training.
    - Transformer has an inductive bias nature, which helps it in transfer.
3. **Ablation studies**: performed three different studies.
    - Without auxiliary LM objective during fine-tuning. Overvation: it helps on the NLI tasks and QQP; larger datasets benefits.
    - Effect of transformer. Observation: $5.6$ average score drop when using LSTM instead of transformer.
    - Compare transformer architecture directly on the supervised target tasks without pre-training. This causes a $14.8%$ decrease from the proposed solution.

### Conclusion
- By pre-training
on a diverse corpus with long stretches of contiguous text, our model acquires significant world
knowledge and ability to process long-range dependencies, which are then successfully transferred to
solving discriminative tasks such as question answering, semantic similarity assessment, entailment
determination, and text classification, improving state of the art on 9 of the $12$ datasets we
study.
- The paper used unsupervised (pre-)training to boost performance.
- The paper suggests significant improvements are possible in the future.
