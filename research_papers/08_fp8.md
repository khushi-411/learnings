# FP8 Formats for Deep Learning

> By ***Paulius Micikevicius, Dusan Stosic, Neil Burgess,
Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha,
Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi,
Stuart Oberman, Mohammad Shoeybi, Michael Siu, Hao Wu***

[Research Paper Link](https://arxiv.org/abs/2209.05433)

### Abstract
- FP8 is used for accelerating deep learning training inference beyond $16$-bit formats.
- Paper presents two encodings:
    - **E4M2**: $4$-bit exponent and $3$-bit mantissa; It has dynamic range. It is not used to represent infinites and has only a one-bit pattern for NaNs.
    - **E5M2**: $5$-bit exponent and $2$-bit mantissa; Follows IEEE $745$ conventions.
- The paper demonstrates the efficiency of FP8 for various deep-learning networks.
- Also, examine FP8 post-training-quantization of language models trained using $16$-bit formats that resisted fixed point int8 quantization.

### Introduction
- Reduce precision representation has helped deep learning training and inference.
- Common dtypes used: TF32, FP16, and BFloat16
- Extreme resources $1$-bit binary applications.
- For inference, fixed-point int8 representation is a popular option.
- Due to non-linear sampling of the real numbers, FP8 will have advantages for inference when compared to int8.
- $5$-bit exponent for training neural networks on CNNs and larger networks.
- This paper describes:
    - $8$-bit binary format for floating point representation using two encodings for FP8.
    - Describes the reasoning behind them.
- Training model using FP8 data types gets the same result as FP16 and BFloat16.

### Aspects of FP8 Usage in Deep Learning
- Dynamic range required by various networks dictates the need for the two formats as well as performance for scaling factor handling in software rather than via exponents bias.
- Mathematical operations on FP8 inputs will produce outputs in higher precision.
- FP8 tensors will be generated by converting to FP8 from wider types (like FP16, FP32 etc).
- Higher precision values need to be multiplied with a scaling factor before casting them to FP8.
    - For better overlap of the corresponding range.
    - It serves the same purpose as loss-scaling serves in mixed-precision training with FP16.
- How to scale factors? General idea: closest to representable magnitude.
- Values that overflow are saturated to the maximum representable value.
- Weight update skipping and reduction of the scaling factor on overflows are not used.
    - These techniques are used by FP16 automatic mixed precision training.
- Values in higher precision get unscaled by multiplying with the inverse of the scaling factor.
- Mechanics of type conversion are orthogonal to the binary format.
    - For E4M3, both infinites and NaNs in a wider range turn into NaNs. Mainly important for mixed-precision training that involves both FP8 and FP16.
    - Non-saturating mode of conversion is required for strict handling.
    - Rounding mode is orthogonal to the interchange format.

### FP8 Binary Interchange Format
- How to use these FP8 encodings?
    - E4M3: weight and activation tensors; (mainly inference and forward pass)
    - E5M2: gradient tensors (in the backward pass of training)
- Follows IEEE $754$ format, deviates only if there is a significant benifit in DL training.
- Straight forward conversion between E5M2 to IEEE FP16 formats.

<div align="center">

|            | E4M3               | E5M2              |
|------------|---------------------|-------------------|
| Exponent bias | $7$                 | $15$                |
| Infinities   | $N/A$               | $S.11111.00_2$       |
| NaN          | $S.1111.111_2$       | $S.11111.\{01, 10, 11\}_2$ |
| Zeros        | $S.0000.000_2$        | $S.00000.00_2$       |
| Max normal   | $S.1111.110_2 = 1.75 ∗ 2^8 = 448$ | $S.11110.11_2 = 1.75 ∗ 2^{15} = 57,344$ |
| Min normal   | $S.0001.000_2 = 2^{-6}$ | $S.00001.00_2 = 2^{-14}$ |
| Max subnorm  | $S.0000.111_2 = 0.875 ∗ 2^{-6}$ | $S.00000.11_2 = 0.75 ∗ 2^{-14}$ |
| Min subnorm  | $S.0000.001_2 = 2^{-9}$ | $S.00000.01_2 = 2^{-16}$ |

</div>

1. **Special value representations**
    - Infinities are not represented.
    - Retain $1$ mantissa bit-pattern for NaNs.
    - For consistency, they retain positive and negative for zero and NaN.
    - IEEE floating point format allows comparison and sorting of floating point values using integer operations.
    - Studies show that $5$ bit exponent provides sufficient per tensor dynamic range for deep learning.
2. **Exponent bias**
    - E4M3 contain $7$, and E5M2 contain $15$ exponent biases.
    - It controls the placement of representable rage on the real number line.
    - Experiments indicate they need pre-tensor adjustment.
    - Pre-tensor scaling to software implementation enables more flexibility.
    - Scaling factor can take on any real value, while programmable bias is equivalent to allowing only powers of $2$ as scaling factors.

### Empirical Results
- Activations, weight tensors are converted to FP8 and than back to wider representations.
- Why wider range representation?
    - different processors may choose different vectors.

#### Training
- All networks were trained on the ImageNet ILSVRC12 dataset.
- All GEMM operations were clipped to FP8.
- Conclude that FP8 training results match of $16$ bit training results.

#### Inference
- $8$ bit inference deployment is greatly simplified by FP8 training. As training and inference use the same dtypes.
- We we use int8, complex steps:
    - First inference in wider representation dtype.
    - To maintain accuracy: sometimes also requires post-training quantization (PTQ).
    - To maintain accuracyL sometimes requires quantization-aware training (QAT).
- In this paper, they evaluated FP8 post-training quantization of models trained in $16$ bit FP.

#### Per-tensor scaling factors
- There are operations where we need to pre-tensor scaling factors are necessary.
- E.g. when using post-training-quantization of a bfloat16-trained network.

### Conclusions
- Introduced FP8 dtype, consisting of E4M3 and E5M2 encodings.
- Accelerates training and reduces the resources required for training. No changes in the model, optimizer, and training hyperparameters.
- Simplifies $8$-bit inference deployment by using the same dtype for training and inference.
- Before the introduction of FP8, using $8$-bit inference or fine-tuning for int8 models trained in floating point increased the complexity of the deployment process; in some cases, accuracy was also affected.
