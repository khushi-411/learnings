# FP8 Formats for Deep Learning

> By ***Paulius Micikevicius, Dusan Stosic, Neil Burgess,
Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha,
Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi,
Stuart Oberman, Mohammad Shoeybi, Michael Siu, Hao Wu***

[Research Paper Link](https://arxiv.org/abs/2209.05433)

### Abstract
- FP8 is used for accelerating deep leaning training inference beyond $16$-bit formats.
- Paper presents two encodings:
    - **E4M2**: $4$-bit exponent and $3$-bit mantissa; It has dynamic range. It is not used to represent inifinites and have only one bit-pattern for NaNs.
    - **E5M2**: $5$-bit exponent and $2$-bit mantissa; Follows IEEE $745$ conventions.
- The paper demonstrates the efficiency of fp8 for various deep learning network.
- Also, examine fp8 post-training-quantization of language models trained using $16$-bit formats that resisted fixed point int8 quantization.

### Introduction
- Reduce precision representation has helped deep learning training and inference.
- Common dtypes used: TF32, FP16 and BFloat16
- Extreme resources $1$-bit binary applications.
- For inference fixed-point int8 representation is a popular option.
- Due to non-linear sampling of the real numbers, FP8 will have advantages for inference when compared to int8.
- $5$-bit exponent for training neural networks, on CNNs and larger networks.
- This paper describes:
    - $8$-bit binary format for floating point representation using two encodings for fp8.
    - Describes the reasoning behind them.
- Training model using fp8 data types gets same result as FP16 and BFloat16.

### Aspects of FP8 Usage in Deep Learning


### Conclusions
- Introduced FP8 dtype, consisting of E4M3 and E5M2 encodings.
- Accelerates training and reduces the resources required for training. No changes in model, optimizer, and training hyperparameters.
- Simplifies $8$-bit inference deployment by using same dtype for training and inference.
- Before the introduction of fp8, using $8$-bit inference or fine-tuning for int8 models trained in floating point increased the complexity to the deployment process, in some cases accuracy was also effected.
